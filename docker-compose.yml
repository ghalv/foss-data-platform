# Docker Compose configuration for FOSS Data Platform

services:
  # JupyterLab for interactive data work
  jupyterlab:
    image: jupyter/datascience-notebook:latest
    container_name: jupyterlab
    ports:
      - "8888:8888"
    environment:
      - JUPYTER_ENABLE_LAB=yes
      - JUPYTER_TOKEN=fossdata123
      - JUPYTER_CONFIG_DIR=/home/jovyan/.jupyter
    volumes:
      - ./pipelines/stavanger_parking/notebooks:/home/jovyan/work
      - ./pipelines/stavanger_parking/dbt:/home/jovyan/pipeline
      - ./data/services:/home/jovyan/data
      - ./config/jupyter:/home/jovyan/.jupyter
    working_dir: /home/jovyan/work
    command: jupyter lab --ServerApp.token='' --ServerApp.ip=0.0.0.0 --ServerApp.allow_root=true --ServerApp.password_required=false
    networks:
      - data-platform
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Dagster for orchestration
  dagster:
    build:
      context: ./platform/orchestration
      dockerfile: Dockerfile
    container_name: dagster
    ports:
      - "3000:3000"
    environment:
      - DAGSTER_HOME=/opt/platform/orchestration/platform/orchestration_home
    volumes:
      - ./platform/orchestration:/opt/platform/orchestration/app
      - ./data/services:/opt/platform/orchestration/data
    working_dir: /opt/platform/orchestration
    command: platform/orchestration-webserver -h 0.0.0.0 -p 3000 -w /opt/platform/orchestration/app/workspace.yaml
    networks:
      - data-platform
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # Apache Trino for querying
  trino-coordinator:
    image: trinodb/trino:latest
    container_name: trino-coordinator
    ports:
      - "8080:8080"
    environment:
      - COORDINATOR=true
      - DISCOVERY_URI=http://trino-coordinator:8080
    volumes:
      - ./config/trino:/etc/trino
      - ./data:/data
    networks:
      - data-platform
    restart: unless-stopped

  trino-worker:
    image: trinodb/trino:latest
    container_name: trino-worker
    environment:
      - COORDINATOR=false
      - DISCOVERY_URI=http://trino-coordinator:8080
      - DISCOVERY=http://trino-coordinator:8080
    volumes:
      - ./config/trino:/etc/trino
      - ./data:/data
    depends_on:
      - trino-coordinator
    networks:
      - data-platform
    restart: unless-stopped

  # MinIO for S3-compatible object storage
  minio:
    image: minio/minio:latest
    container_name: minio
    ports:
      - "9002:9000"
      - "9003:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-admin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-CHANGE_THIS_PASSWORD}
    volumes:
      - ./data/services/minio:/data
      - ./config/minio:/scripts
      - ./config/minio:/etc/minio
    command: server /data --console-address ":9001" --anonymous
    networks:
      - data-platform
    restart: unless-stopped

  # Hive Metastore for Iceberg catalog
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: hive-metastore
    ports:
      - "9083:9083"
    environment:
      - SERVICE_NAME=hive-metastore
      - HIVE_METASTORE_PORT=9083
      - HIVE_METASTORE_DB_HOST=postgres
      - HIVE_METASTORE_DB_PORT=5432
      - HIVE_METASTORE_DB_NAME=hive_metastore
      - HIVE_METASTORE_DB_USER=platform/orchestration
      - HIVE_METASTORE_DB_PASSWORD=platform/orchestration123
    volumes:
      - ./config/hive:/opt/hive/conf
      - ./data/services/hive:/opt/hive/data
    entrypoint: ["/opt/hive/conf/init-hive.sh"]
    depends_on:
      - postgres
    networks:
      - data-platform
    restart: unless-stopped

  # Prometheus for monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus:/etc/prometheus
      - ./data/services/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - data-platform
    restart: unless-stopped

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:-CHANGE_THIS_PASSWORD}
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning
      - ./data/services/grafana:/var/lib/grafana
    networks:
      - data-platform
    restart: unless-stopped

  # PostgreSQL for metadata storage
  postgres:
    image: postgres:15
    container_name: postgres
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_DB=platform/orchestration
      - POSTGRES_USER=platform/orchestration
      - POSTGRES_PASSWORD=platform/orchestration123
    volumes:
      - ./data/services/postgres:/var/run/postgresql/data
      - ./config/postgres:/docker-entrypoint-initdb.d
    networks:
      - data-platform
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dagster -d dagster"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Hive Metastore for Iceberg metadata (temporarily disabled - schema initialization issues)
  # hive-metastore:
  #   image: apache/hive:3.1.3
  #   container_name: hive-metastore
  #   environment:
  #     - SERVICE_NAME=metastore
  #     - DB_DRIVER=postgres
  #     - DB_HOST=postgres
  #     - DB_PORT=5432
  #     - DB_NAME=metastore
  #     - DB_USER=metastore
  #     - DB_PASS=metastore123
  #     - HIVE_METASTORE_SCHEMA_VERIFICATION=false
  #     - DATANUCLEUS_AUTO_CREATE_SCHEMA=true
  #     - DATANUCLEUS_AUTO_CREATE_TABLES=true
  #   ports:
  #     - "9083:9083"
  #   volumes:
  #     - ./data/services/hive:/opt/hive/data
  #   networks:
  #     - data-platform
  #   restart: unless-stopped
  #   depends_on:
  #     postgres:
  #       condition: service_healthy

  # Redis for caching
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6380:6379"
    volumes:
      - ./data/services/redis:/data
    networks:
      - data-platform
    restart: unless-stopped

  # Platform Dashboard
  dashboard:
    build: ./dashboard
    container_name: dashboard
    ports:
      - "5000:5000"
    env_file:
      - .env
    volumes:
      - ./dashboard:/app
      - ./pipelines/stavanger_parking/dbt:/app/pipelines/stavanger_parking/dbt
      - ./.venv:/app/.venv
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
    environment:
      - FLASK_ENV=development
      - FLASK_APP=app.py
    networks:
      - data-platform
    restart: unless-stopped

  # Portainer for container management
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - "9000:9000"
    environment:
      - PORTAINER_ADMIN_PASSWORD=${PORTAINER_ADMIN_PASSWORD:-CHANGE_THIS_PASSWORD}
      - PORTAINER_EDGE=0
      - PORTAINER_EDGE_INSECURE_POLL=0
      - PORTAINER_EDGE_ASYNC=0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./data/services/portainer:/data
      - ./data/services/portainer:/var/lib/portainer
    networks:
      - data-platform
    restart: unless-stopped



  # Real-time Streaming Infrastructure
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - ./data/services/zookeeper/data:/var/lib/zookeeper/data
      - ./data/services/zookeeper/log:/var/lib/zookeeper/log
    networks:
      - data-platform
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_JMX_PORT: 9101
    volumes:
      - ./data/services/kafka:/var/lib/kafka/data
    networks:
      - data-platform
    restart: unless-stopped

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    container_name: kafka-ui
    depends_on:
      - kafka
    ports:
      - "8082:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:29092
      KAFKA_CLUSTERS_0_ZOOKEEPER: zookeeper:2181
    networks:
      - data-platform
    restart: unless-stopped

  # Stream Processing
  flink-jobmanager:
    image: apache/flink:1.18.1
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.rpc.port: 6123
        jobmanager.memory.process.size: 1600m
        taskmanager.memory.process.size: 1728m
        taskmanager.numberOfTaskSlots: 1
        parallelism.default: 1
        state.backend: filesystem
        state.checkpoints.dir: file:///checkpoints
        state.savepoints.dir: file:///savepoints
    volumes:
      - ./data/services/flink/checkpoints:/checkpoints
      - ./data/services/flink/savepoints:/savepoints
    networks:
      - data-platform
    restart: unless-stopped

  flink-taskmanager:
    image: apache/flink:1.18.1
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    command: taskmanager
    environment:
      - |
        FLINK_PROPERTIES=
        jobmanager.rpc.address: flink-jobmanager
        jobmanager.rpc.port: 6123
        jobmanager.memory.process.size: 1600m
        taskmanager.memory.process.size: 1728m
        taskmanager.numberOfTaskSlots: 1
        parallelism.default: 1
        state.backend: filesystem
        state.checkpoints.dir: file:///checkpoints
        state.savepoints.dir: file:///savepoints
    volumes:
      - ./data/services/flink/checkpoints:/checkpoints
      - ./data/services/flink/savepoints:/savepoints
    networks:
      - data-platform
    restart: unless-stopped

  # Automated Cleanup Service
  cleanup-scheduler:
    image: python:3.9-alpine
    container_name: cleanup-scheduler
    depends_on:
      - postgres
      - dashboard
    volumes:
      - ./scripts:/scripts
      - ./config/postgres:/config/postgres
    command: >
      sh -c "
        apk add --no-cache postgresql-client &&
        pip install psycopg2-binary &&
        echo '0 2 * * * python /scripts/cleanup_operations.py --run --type automatic --db-host postgres --db-port 5432' > /etc/crontabs/root &&
        echo 'Starting cleanup scheduler...' &&
        crond -f -l 8
      "
    networks:
      - data-platform
    restart: unless-stopped

networks:
  data-platform:
    driver: bridge

volumes:
  data:
    driver: local
